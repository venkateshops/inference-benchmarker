

imagePullSecrets: [ ]
nameOverride: ""
fullnameOverride: ""

hf_token: ""
model_id: "meta-llama/Llama-3.1-8B-Instruct"
server: tgi

tgi:
  enabled: true
  extra_args:
    - "--max-concurrent-requests"
    - "512"
  image:
    repository: ghcr.io/huggingface/text-generation-inference
    pullPolicy: IfNotPresent
    tag: "latest"
  replicaCount: 1
  resources:
    limits:
      "nvidia.com/gpu": "1"
  podAnnotations: { }
  podLabels: { }
  podSecurityContext: { }
  securityContext: { }
  nodeSelector: { }
  tolerations: [ ]
  affinity: { }

vllm:
  enabled: false
  extra_args:
  image:
    repository: vllm/vllm-openai
    pullPolicy: IfNotPresent
    tag: "latest"
  replicaCount: 1
  resources:
    limits:
      "nvidia.com/gpu": "1"
  podAnnotations: { }
  podLabels: { }
  podSecurityContext: { }
  securityContext: { }
  nodeSelector: { }
  tolerations: [ ]
  affinity: { }

benchmark:
  extra_args:
    - "--max-vus"
    - "800"
    - "--duration"
    - "120s"
    - "--warmup"
    - "30s"
    - "--benchmark-kind"
    - "sweep"
  image:
    repository: ghcr.io/huggingface/text-generation-inference-benchmark
    pullPolicy: IfNotPresent
    tag: "latest"
  podAnnotations: { }
  podLabels: { }
  podSecurityContext: { }
  securityContext: { }
  resources: { }
  nodeSelector: { }
  tolerations: [ ]
  affinity: { }



